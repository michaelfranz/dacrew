# yaml-language-server: $schema=https://raw.githubusercontent.com/microsoft/vscode/main/extensions/github-actions/schemas/github-workflow.json

name: Python Tests

on:
  push:
    branches:
      - main
      - 'feature/*'
      - 'bugfix/*'
      - 'hotfix/*'
  pull_request:
    branches:
      - main
      - 'feature/*'

env:
  PYTHON_VERSION: '3.10'

jobs:
  test:
    runs-on: ubuntu-latest
    timeout-minutes: 15  # Add overall job timeout

    strategy:
      matrix:
        python-version: ['3.10']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'

      # Step to handle dependency conflicts
      - name: Install dependencies with conflict resolution
        run: |
          python -m pip install --upgrade pip
          # Try to install requirements with dependency resolution
          pip install -r requirements.txt || {
            echo "Direct installation failed, trying with --upgrade-strategy eager"
            pip install -r requirements.txt --upgrade-strategy eager
          } || {
            echo "Installation with eager strategy failed, trying individual packages"
            # Install core packages first
            pip install pytest pytest-cov pytest-html pytest-xdist timeout-decorator
            # Install main dependencies with latest compatible versions
            pip install --upgrade crewai crewai-tools langchain langchain-cohere langchain-community
            pip install --upgrade langchain-core langchain-experimental langchain-openai langchain-text-splitters
            pip install --upgrade openai jira atlassian-python-api
            # Install AI/ML packages - let pip resolve chromadb version conflicts
            pip install --upgrade sentence-transformers faiss-cpu transformers torch datasets
            pip install pandas numpy python-dotenv click rich typer black isort mypy pydantic setuptools
          }

      - name: Verify installation and show package versions
        run: |
          pip list | grep -E "(crewai|chromadb|langchain|pytest)"
          python -c "import crewai; print(f'CrewAI version: {crewai.__version__}')" || echo "CrewAI import failed"
          python -c "import chromadb; print(f'ChromaDB version: {chromadb.__version__}')" || echo "ChromaDB import failed"

      # Only run tests if src directory exists and has Python files
      - name: Check for source code
        id: check_src
        run: |
          if [ -d "src" ] && find src -name "*.py" -type f | head -1 | grep -q .; then
            echo "has_src=true" >> $GITHUB_OUTPUT
          else
            echo "has_src=false" >> $GITHUB_OUTPUT
            echo "No Python source files found in src directory"
          fi

      - name: Run unit tests with timeout
        if: steps.check_src.outputs.has_src == 'true'
        timeout-minutes: 10  # Add timeout for this step
        run: |
          # Check if unit tests exist
          if [ -d "tests/unit" ] && find tests/unit -name "test_*.py" -o -name "*_test.py" | head -1 | grep -q .; then
            echo "Running unit tests with timeout protection..."
          
            # Use timeout command and pytest timeout plugin
            timeout 600s pytest tests/unit/ \
              --timeout=30 \
              --timeout-method=thread \
              --junitxml=test-results-unit.xml \
              --cov=src \
              --cov-report=xml:coverage-unit.xml \
              --cov-report=html:htmlcov-unit \
              --cov-report=term-missing \
              --maxfail=5 \
              -v \
              -x || {
                echo "Unit tests failed or timed out"
                # Still create results file for upload
                [ ! -f test-results-unit.xml ] && echo '<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="unit-tests" tests="0" failures="1" errors="1" skipped="0"><testcase name="timeout" classname="UnitTests"><failure message="Tests timed out or failed">Unit tests timed out or failed to complete</failure></testcase></testsuite></testsuites>' > test-results-unit.xml
                exit 1
              }
          else
            echo "No unit tests found in tests/unit directory"
            echo '<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="unit-tests" tests="0" failures="0" errors="0" skipped="0"></testsuite></testsuites>' > test-results-unit.xml
          fi

      - name: Run integration tests with timeout
        if: steps.check_src.outputs.has_src == 'true'
        timeout-minutes: 8
        continue-on-error: true  # Don't fail the entire job if integration tests timeout
        run: |
          if [ -d "tests/integration" ] && find tests/integration -name "test_*.py" -o -name "*_test.py" | head -1 | grep -q .; then
            echo "Running integration tests with timeout protection..."
          
            timeout 480s pytest tests/integration/ \
              --timeout=60 \
              --timeout-method=thread \
              --junitxml=test-results-integration.xml \
              --cov=src \
              --cov-append \
              --cov-report=xml:coverage-integration.xml \
              --cov-report=html:htmlcov-integration \
              --maxfail=3 \
              -v \
              -x || {
                echo "Integration tests failed or timed out"
                [ ! -f test-results-integration.xml ] && echo '<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="integration-tests" tests="0" failures="1" errors="1" skipped="0"><testcase name="timeout" classname="IntegrationTests"><failure message="Tests timed out or failed">Integration tests timed out or failed to complete</failure></testcase></testsuite></testsuites>' > test-results-integration.xml
              }
          else
            echo "No integration tests found in tests/integration directory"
            echo '<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="integration-tests" tests="0" failures="0" errors="0" skipped="0"></testsuite></testsuites>' > test-results-integration.xml
          fi

      - name: Run end-to-end tests with timeout
        if: steps.check_src.outputs.has_src == 'true'
        timeout-minutes: 8
        continue-on-error: true
        run: |
          if [ -d "tests/e2e" ] && find tests/e2e -name "test_*.py" -o -name "*_test.py" | head -1 | grep -q .; then
            echo "Running e2e tests with timeout protection..."
          
            timeout 480s pytest tests/e2e/ \
              --timeout=60 \
              --timeout-method=thread \
              --junitxml=test-results-e2e.xml \
              --cov=src \
              --cov-append \
              --cov-report=xml:coverage-e2e.xml \
              --cov-report=html:htmlcov-e2e \
              --maxfail=3 \
              -v \
              -x || {
                echo "E2E tests failed or timed out"
                [ ! -f test-results-e2e.xml ] && echo '<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="e2e-tests" tests="0" failures="1" errors="1" skipped="0"><testcase name="timeout" classname="E2ETests"><failure message="Tests timed out or failed">E2E tests timed out or failed to complete</failure></testcase></testsuite></testsuites>' > test-results-e2e.xml
              }
          else
            echo "No e2e tests found in tests/e2e directory"
            echo '<?xml version="1.0" encoding="utf-8"?><testsuites><testsuite name="e2e-tests" tests="0" failures="0" errors="0" skipped="0"></testsuite></testsuites>' > test-results-e2e.xml
          fi

      # Run a quick smoke test to identify problematic tests
      - name: Run problematic test identification
        if: failure()
        continue-on-error: true
        run: |
          echo "Identifying potentially problematic tests..."
          
          # Run each test file individually with short timeout to identify hanging tests
          find tests/unit -name "test_*.py" | while read test_file; do
            echo "Testing $test_file individually..."
            timeout 60s pytest "$test_file" -v --tb=short || echo "PROBLEMATIC: $test_file"
          done

      - name: Upload test results and artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.python-version }}
          path: |
            test-results-*.xml
            coverage-*.xml
            htmlcov-*/**
          retention-days: 30

      # Optional: Create a summary report
      - name: Test Summary
        if: always()
        run: |
          echo "## Test Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Count test files
          UNIT_TESTS=$(find tests/unit -name "test_*.py" -o -name "*_test.py" 2>/dev/null | wc -l || echo "0")
          INTEGRATION_TESTS=$(find tests/integration -name "test_*.py" -o -name "*_test.py" 2>/dev/null | wc -l || echo "0")
          E2E_TESTS=$(find tests/e2e -name "test_*.py" -o -name "*_test.py" 2>/dev/null | wc -l || echo "0")
          
          echo "- Unit test files: $UNIT_TESTS" >> $GITHUB_STEP_SUMMARY
          echo "- Integration test files: $INTEGRATION_TESTS" >> $GITHUB_STEP_SUMMARY
          echo "- E2E test files: $E2E_TESTS" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Show test results if they exist
          if [ -f test-results-unit.xml ]; then
            echo "### Unit Test Results" >> $GITHUB_STEP_SUMMARY
            grep -o 'tests="[^"]*"' test-results-unit.xml | head -1 >> $GITHUB_STEP_SUMMARY || echo "Could not parse test results" >> $GITHUB_STEP_SUMMARY
          fi
          
          pip list | grep -E "(crewai|chromadb|langchain|pytest)" >> $GITHUB_STEP_SUMMARY || echo "Package list unavailable" >> $GITHUB_STEP_SUMMARY